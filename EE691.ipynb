{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('raw_dat_1.csv')\n",
    "\n",
    "columns_to_delete = list(df.columns[:3]) + list(df.columns[8:])\n",
    "df = df.drop(columns=columns_to_delete, axis=1)\n",
    "df = df.iloc[:-400]\n",
    "\n",
    "output_file = 'raw_dat_v2.csv'\n",
    "df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02,  8.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost time: 2.9422271251678467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.91it/s]\n",
      "7it [00:00, 17.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Train Loss: 0.9701965 Vali Loss: 0.4774393 Test Loss: 0.5988588 MAE Loss: 0.6146013\n",
      "lr = 0.0000400000\n",
      "Updating learning rate to 3.9999999999999996e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 cost time: 2.396179437637329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.35it/s]\n",
      "7it [00:00, 18.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Train Loss: 0.6810757 Vali Loss: 0.3558971 Test Loss: 0.4557828 MAE Loss: 0.5235018\n",
      "Updating learning rate to 1.9999999999999998e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 cost time: 2.4041898250579834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.58it/s]\n",
      "7it [00:00, 17.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Train Loss: 0.5981365 Vali Loss: 0.3306210 Test Loss: 0.4294877 MAE Loss: 0.5072071\n",
      "Updating learning rate to 9.999999999999999e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 cost time: 2.406959056854248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.46it/s]\n",
      "7it [00:00, 18.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Train Loss: 0.5562807 Vali Loss: 0.3162358 Test Loss: 0.4115965 MAE Loss: 0.4951081\n",
      "Updating learning rate to 4.9999999999999996e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 cost time: 2.3816874027252197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.81it/s]\n",
      "7it [00:00, 17.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Train Loss: 0.5421082 Vali Loss: 0.3105375 Test Loss: 0.4039805 MAE Loss: 0.4894628\n",
      "Updating learning rate to 2.4999999999999998e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 cost time: 2.399993658065796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.32it/s]\n",
      "7it [00:00, 18.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Train Loss: 0.5346692 Vali Loss: 0.3079509 Test Loss: 0.4011889 MAE Loss: 0.4874439\n",
      "Updating learning rate to 1.2499999999999999e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 cost time: 2.419506311416626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.32it/s]\n",
      "7it [00:00, 17.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Train Loss: 0.5317082 Vali Loss: 0.3066886 Test Loss: 0.3998954 MAE Loss: 0.4865555\n",
      "Updating learning rate to 6.249999999999999e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 cost time: 2.389252185821533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.40it/s]\n",
      "7it [00:00, 18.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Train Loss: 0.5272254 Vali Loss: 0.3062128 Test Loss: 0.3993007 MAE Loss: 0.4861921\n",
      "Updating learning rate to 3.1249999999999997e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 cost time: 2.441629409790039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.74it/s]\n",
      "7it [00:00, 17.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Train Loss: 0.5315284 Vali Loss: 0.3059555 Test Loss: 0.3989859 MAE Loss: 0.4859967\n",
      "Updating learning rate to 1.5624999999999999e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 cost time: 2.423584461212158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.49it/s]\n",
      "7it [00:00, 17.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Train Loss: 0.5280101 Vali Loss: 0.3058090 Test Loss: 0.3988362 MAE Loss: 0.4858945\n",
      "Updating learning rate to 7.812499999999999e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 cost time: 2.4644508361816406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.38it/s]\n",
      "7it [00:00, 17.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Train Loss: 0.5290949 Vali Loss: 0.3057413 Test Loss: 0.3987709 MAE Loss: 0.4858509\n",
      "Updating learning rate to 3.9062499999999997e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 cost time: 2.4115428924560547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.36it/s]\n",
      "7it [00:00, 17.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Train Loss: 0.5218086 Vali Loss: 0.3057060 Test Loss: 0.3987352 MAE Loss: 0.4858246\n",
      "Updating learning rate to 1.9531249999999998e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 cost time: 2.428776979446411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.34it/s]\n",
      "7it [00:00, 17.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Train Loss: 0.5354246 Vali Loss: 0.3056887 Test Loss: 0.3987136 MAE Loss: 0.4858104\n",
      "Updating learning rate to 9.765624999999999e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 cost time: 2.4282145500183105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.78it/s]\n",
      "7it [00:00, 17.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Train Loss: 0.5336878 Vali Loss: 0.3056797 Test Loss: 0.3987024 MAE Loss: 0.4858026\n",
      "Updating learning rate to 4.8828124999999996e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 cost time: 2.4506964683532715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.40it/s]\n",
      "7it [00:00, 17.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Train Loss: 0.5273545 Vali Loss: 0.3056753 Test Loss: 0.3986969 MAE Loss: 0.4857985\n",
      "Updating learning rate to 2.4414062499999998e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 cost time: 2.461622476577759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.52it/s]\n",
      "7it [00:00, 17.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Train Loss: 0.5266860 Vali Loss: 0.3056738 Test Loss: 0.3986950 MAE Loss: 0.4857971\n",
      "Updating learning rate to 1.2207031249999999e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 cost time: 2.4513208866119385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.52it/s]\n",
      "7it [00:00, 17.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Train Loss: 0.5195248 Vali Loss: 0.3056733 Test Loss: 0.3986945 MAE Loss: 0.4857967\n",
      "Updating learning rate to 6.103515624999999e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 cost time: 2.457735300064087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.54it/s]\n",
      "7it [00:00, 18.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Train Loss: 0.5238062 Vali Loss: 0.3056732 Test Loss: 0.3986944 MAE Loss: 0.4857966\n",
      "Updating learning rate to 3.0517578124999997e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 cost time: 2.426872491836548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.90it/s]\n",
      "7it [00:00, 17.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Train Loss: 0.5236541 Vali Loss: 0.3056731 Test Loss: 0.3986943 MAE Loss: 0.4857966\n",
      "Updating learning rate to 1.5258789062499999e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 cost time: 2.449314594268799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.47it/s]\n",
      "7it [00:00, 17.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Train Loss: 0.5251916 Vali Loss: 0.3056731 Test Loss: 0.3986943 MAE Loss: 0.4857966\n",
      "Updating learning rate to 7.629394531249999e-11\n",
      "success delete checkpoints\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import argparse\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from models import Autoformer, DLinear, TimeLLM\n",
    "\n",
    "#from data_provider.data_factory import data_provider\n",
    "import time\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, num_features: int, eps=1e-5, affine=False, subtract_last=False, non_norm=False):\n",
    "        \"\"\"\n",
    "        :param num_features: the number of features or channels\n",
    "        :param eps: a value added for numerical stability\n",
    "        :param affine: if True, RevIN has learnable affine parameters\n",
    "        \"\"\"\n",
    "        super(Normalize, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        self.subtract_last = subtract_last\n",
    "        self.non_norm = non_norm\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def forward(self, x, mode: str):\n",
    "        if mode == 'norm':\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == 'denorm':\n",
    "            x = self._denormalize(x)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return x\n",
    "\n",
    "    def _init_params(self):\n",
    "        # initialize RevIN params: (C,)\n",
    "        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        dim2reduce = tuple(range(1, x.ndim - 1))\n",
    "        if self.subtract_last:\n",
    "            self.last = x[:, -1, :].unsqueeze(1)\n",
    "        else:\n",
    "            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        if self.non_norm:\n",
    "            return x\n",
    "        if self.subtract_last:\n",
    "            x = x - self.last\n",
    "        else:\n",
    "            x = x - self.mean\n",
    "        x = x / self.stdev\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight\n",
    "            x = x + self.affine_bias\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.non_norm:\n",
    "            return x\n",
    "        if self.affine:\n",
    "            x = x - self.affine_bias\n",
    "            x = x / (self.affine_weight + self.eps * self.eps)\n",
    "        x = x * self.stdev\n",
    "        if self.subtract_last:\n",
    "            x = x + self.last\n",
    "        else:\n",
    "            x = x + self.mean\n",
    "        return x\n",
    "\n",
    "class Dataset_Custom(Dataset):\n",
    "    def __init__(self, root_path, flag='train', size=None,\n",
    "                 features='S', data_path='ETTh1.csv',\n",
    "                 target='OT', scale=True,\n",
    "                 #timeenc=0, freq='h',\n",
    "                 percent=100\n",
    "                 #seasonal_patterns=None\n",
    "                 ):\n",
    "        if size == None:\n",
    "            self.seq_len = 24 * 4 * 4\n",
    "            self.label_len = 24 * 4\n",
    "            self.pred_len = 24 * 4\n",
    "        else:\n",
    "            self.seq_len = size[0]\n",
    "            self.label_len = size[1]\n",
    "            self.pred_len = size[2]\n",
    "        # init\n",
    "        assert flag in ['train', 'test', 'val']\n",
    "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "        self.set_type = type_map[flag]\n",
    "\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.scale = scale\n",
    "        #self.timeenc = timeenc\n",
    "        #self.freq = freq\n",
    "        self.percent = percent\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.data_path = data_path\n",
    "        self.__read_data__()\n",
    "\n",
    "        self.enc_in = self.data_x.shape[-1]\n",
    "        self.tot_len = len(self.data_x) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def __read_data__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
    "                                          self.data_path))\n",
    "\n",
    "        '''\n",
    "        df_raw.columns: ['date', ...(other features), target feature]\n",
    "        '''\n",
    "        cols = list(df_raw.columns)\n",
    "        cols.remove(self.target)\n",
    "        #cols.remove('date')\n",
    "        #df_raw = df_raw[['date'] + cols + [self.target]]\n",
    "        df_raw = df_raw[cols + [self.target]]\n",
    "        num_train = int(len(df_raw) * 0.7)\n",
    "        num_test = int(len(df_raw) * 0.2)\n",
    "        num_vali = len(df_raw) - num_train - num_test\n",
    "        border1s = [0, num_train - self.seq_len, len(df_raw) - num_test - self.seq_len]\n",
    "        border2s = [num_train, num_train + num_vali, len(df_raw)]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "\n",
    "        if self.set_type == 0:\n",
    "            border2 = (border2 - self.seq_len) * self.percent // 100 + self.seq_len\n",
    "\n",
    "        if self.features == 'M' or self.features == 'MS':\n",
    "            cols_data = df_raw.columns[1:]\n",
    "            df_data = df_raw[cols_data]\n",
    "        elif self.features == 'S':\n",
    "            df_data = df_raw[[self.target]]\n",
    "\n",
    "        if self.scale:\n",
    "            train_data = df_data[border1s[0]:border2s[0]]\n",
    "            self.scaler.fit(train_data.values)\n",
    "            data = self.scaler.transform(df_data.values)\n",
    "        else:\n",
    "            data = df_data.values\n",
    "\n",
    "        '''df_stamp = df_raw[['date']][border1:border2]\n",
    "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
    "        if self.timeenc == 0:\n",
    "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
    "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
    "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
    "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
    "            data_stamp = df_stamp.drop(['date'], 1).values\n",
    "        elif self.timeenc == 1:\n",
    "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
    "            data_stamp = data_stamp.transpose(1, 0)\n",
    "        '''\n",
    "        self.data_x = data[border1:border2]\n",
    "        self.data_y = data[border1:border2]\n",
    "        #self.data_stamp = data_stamp\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feat_id = index // self.tot_len\n",
    "        s_begin = index % self.tot_len\n",
    "\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "        seq_x = self.data_x[s_begin:s_end, feat_id:feat_id + 1]\n",
    "        seq_y = self.data_y[r_begin:r_end, feat_id:feat_id + 1]\n",
    "        #seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "        #seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "\n",
    "        return seq_x, seq_y #, seq_x_mark, seq_y_mark\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data_x) - self.seq_len - self.pred_len + 1) * self.enc_in\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)\n",
    "\n",
    "def data_provider(args, flag):\n",
    "    Data = Dataset_Custom\n",
    "    #timeenc = 0 if args.embed != 'timeF' else 1\n",
    "    percent = args.percent\n",
    "\n",
    "    if flag == 'test':\n",
    "        shuffle_flag = False\n",
    "        drop_last = True\n",
    "        batch_size = args.batch_size\n",
    "        #freq = args.freq\n",
    "    else:\n",
    "        shuffle_flag = True\n",
    "        drop_last = True\n",
    "        batch_size = args.batch_size\n",
    "        #freq = args.freq\n",
    "\n",
    "    data_set = Data(\n",
    "            root_path=args.root_path,\n",
    "            data_path=args.data_path,\n",
    "            flag=flag,\n",
    "            size=[args.seq_len, args.label_len, args.pred_len],\n",
    "            features=args.features,\n",
    "            target=args.target,\n",
    "            #timeenc=timeenc,\n",
    "            #freq=freq,\n",
    "            percent=percent\n",
    "            #seasonal_patterns=args.seasonal_patterns\n",
    "        )\n",
    "    data_loader = DataLoader(\n",
    "        data_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle_flag,\n",
    "        num_workers=args.num_workers,\n",
    "        drop_last=drop_last)\n",
    "    return data_set, data_loader\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1 if torch.__version__ >= '1.5.0' else 2\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
    "                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ReplicationPad1d(nn.Module):\n",
    "    def __init__(self, padding) -> None:\n",
    "        super(ReplicationPad1d, self).__init__()\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        replicate_padding = input[:, :, -1].unsqueeze(-1).repeat(1, 1, self.padding[-1])\n",
    "        output = torch.cat([input, replicate_padding], dim=-1)\n",
    "        return output\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, patch_len, stride, dropout):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        # Patching\n",
    "        self.patch_len = patch_len\n",
    "        self.stride = stride\n",
    "        self.padding_patch_layer = ReplicationPad1d((0, stride))\n",
    "\n",
    "        # Backbone, Input encoding: projection of feature vectors onto a d-dim vector space\n",
    "        self.value_embedding = TokenEmbedding(patch_len, d_model)\n",
    "\n",
    "        # Positional embedding\n",
    "        # self.position_embedding = PositionalEmbedding(d_model)\n",
    "\n",
    "        # Residual dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # do patching\n",
    "        n_vars = x.shape[1]\n",
    "        x = self.padding_patch_layer(x)\n",
    "        x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)\n",
    "        x = torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2], x.shape[3]))\n",
    "        # Input encoding\n",
    "        x = self.value_embedding(x)\n",
    "        return self.dropout(x), n_vars\n",
    "\n",
    "#from math import sqrt\n",
    "\n",
    "#import torch\n",
    "#import torch.nn as nn\n",
    "\n",
    "from transformers import LlamaConfig, LlamaModel, LlamaTokenizer, GPT2Config, GPT2Model, GPT2Tokenizer, BertConfig, \\\n",
    "    BertModel, BertTokenizer\n",
    "#from layers.Embed import PatchEmbedding\n",
    "import transformers\n",
    "#from layers.StandardNorm import Normalize\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "class FlattenHead(nn.Module):\n",
    "    def __init__(self, n_vars, nf, target_window, head_dropout=0):\n",
    "        super().__init__()\n",
    "        self.n_vars = n_vars\n",
    "        self.flatten = nn.Flatten(start_dim=-2)\n",
    "        self.linear = nn.Linear(nf, target_window)\n",
    "        self.dropout = nn.Dropout(head_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, configs, patch_len=16, stride=8):\n",
    "        super(Model, self).__init__()\n",
    "        self.task_name = configs.task_name\n",
    "        self.pred_len = configs.pred_len\n",
    "        self.seq_len = configs.seq_len\n",
    "        self.d_ff = configs.d_ff\n",
    "        self.top_k = 5\n",
    "        self.d_llm = configs.llm_dim\n",
    "        self.patch_len = configs.patch_len\n",
    "        self.stride = configs.stride\n",
    "\n",
    "        if configs.llm_model == 'LLAMA':\n",
    "            # self.llama_config = LlamaConfig.from_pretrained('/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/')\n",
    "            self.llama_config = LlamaConfig.from_pretrained('huggyllama/llama-7b')\n",
    "            self.llama_config.num_hidden_layers = configs.llm_layers\n",
    "            self.llama_config.output_attentions = True\n",
    "            self.llama_config.output_hidden_states = True\n",
    "            try:\n",
    "                self.llm_model = LlamaModel.from_pretrained(\n",
    "                    # \"/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/\",\n",
    "                    'huggyllama/llama-7b',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True,\n",
    "                    config=self.llama_config,\n",
    "                    # load_in_4bit=True\n",
    "                )\n",
    "            except EnvironmentError:  # downloads model from HF is not already done\n",
    "                print(\"Local model files not found. Attempting to download...\")\n",
    "                self.llm_model = LlamaModel.from_pretrained(\n",
    "                    # \"/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/\",\n",
    "                    'huggyllama/llama-7b',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False,\n",
    "                    config=self.llama_config,\n",
    "                    # load_in_4bit=True\n",
    "                )\n",
    "            try:\n",
    "                self.tokenizer = LlamaTokenizer.from_pretrained(\n",
    "                    # \"/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/tokenizer.model\",\n",
    "                    'huggyllama/llama-7b',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True\n",
    "                )\n",
    "            except EnvironmentError:  # downloads the tokenizer from HF if not already done\n",
    "                print(\"Local tokenizer files not found. Atempting to download them..\")\n",
    "                self.tokenizer = LlamaTokenizer.from_pretrained(\n",
    "                    # \"/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/tokenizer.model\",\n",
    "                    'huggyllama/llama-7b',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False\n",
    "                )\n",
    "        elif configs.llm_model == 'GPT2':\n",
    "            self.gpt2_config = GPT2Config.from_pretrained('openai-community/gpt2')\n",
    "\n",
    "            self.gpt2_config.num_hidden_layers = configs.llm_layers\n",
    "            self.gpt2_config.output_attentions = True\n",
    "            self.gpt2_config.output_hidden_states = True\n",
    "            try:\n",
    "                self.llm_model = GPT2Model.from_pretrained(\n",
    "                    'openai-community/gpt2',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True,\n",
    "                    config=self.gpt2_config,\n",
    "                )\n",
    "            except EnvironmentError:  # downloads model from HF is not already done\n",
    "                print(\"Local model files not found. Attempting to download...\")\n",
    "                self.llm_model = GPT2Model.from_pretrained(\n",
    "                    'openai-community/gpt2',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False,\n",
    "                    config=self.gpt2_config,\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                self.tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "                    'openai-community/gpt2',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True\n",
    "                )\n",
    "            except EnvironmentError:  # downloads the tokenizer from HF if not already done\n",
    "                print(\"Local tokenizer files not found. Atempting to download them..\")\n",
    "                self.tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "                    'openai-community/gpt2',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False\n",
    "                )\n",
    "        elif configs.llm_model == 'BERT':\n",
    "            self.bert_config = BertConfig.from_pretrained('google-bert/bert-base-uncased')\n",
    "\n",
    "            self.bert_config.num_hidden_layers = configs.llm_layers\n",
    "            self.bert_config.output_attentions = True\n",
    "            self.bert_config.output_hidden_states = True\n",
    "            try:\n",
    "                self.llm_model = BertModel.from_pretrained(\n",
    "                    'google-bert/bert-base-uncased',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True,\n",
    "                    config=self.bert_config,\n",
    "                )\n",
    "            except EnvironmentError:  # downloads model from HF is not already done\n",
    "                print(\"Local model files not found. Attempting to download...\")\n",
    "                self.llm_model = BertModel.from_pretrained(\n",
    "                    'google-bert/bert-base-uncased',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False,\n",
    "                    config=self.bert_config,\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                self.tokenizer = BertTokenizer.from_pretrained(\n",
    "                    'google-bert/bert-base-uncased',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True\n",
    "                )\n",
    "            except EnvironmentError:  # downloads the tokenizer from HF if not already done\n",
    "                print(\"Local tokenizer files not found. Atempting to download them..\")\n",
    "                self.tokenizer = BertTokenizer.from_pretrained(\n",
    "                    'google-bert/bert-base-uncased',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False\n",
    "                )\n",
    "        else:\n",
    "            raise Exception('LLM model is not defined')\n",
    "\n",
    "        if self.tokenizer.eos_token:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        else:\n",
    "            pad_token = '[PAD]'\n",
    "            self.tokenizer.add_special_tokens({'pad_token': pad_token})\n",
    "            self.tokenizer.pad_token = pad_token\n",
    "\n",
    "        for param in self.llm_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        if configs.prompt_domain:\n",
    "            self.description = configs.content\n",
    "        else:\n",
    "            #self.description = 'The Electricity Transformer Temperature (ETT) is a crucial indicator in the electric power long-term deployment.'\n",
    "            self.description = 'Weekly weather data is a crucial indicator in pest prediction models, crucial for effective pest management strategies.'\n",
    "\n",
    "        self.dropout = nn.Dropout(configs.dropout)\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            configs.d_model, self.patch_len, self.stride, configs.dropout)\n",
    "\n",
    "        self.word_embeddings = self.llm_model.get_input_embeddings().weight\n",
    "        self.vocab_size = self.word_embeddings.shape[0]\n",
    "        self.num_tokens = 1000\n",
    "        self.mapping_layer = nn.Linear(self.vocab_size, self.num_tokens)\n",
    "\n",
    "        self.reprogramming_layer = ReprogrammingLayer(configs.d_model, configs.n_heads, self.d_ff, self.d_llm)\n",
    "\n",
    "        self.patch_nums = int((configs.seq_len - self.patch_len) / self.stride + 2)\n",
    "        self.head_nf = self.d_ff * self.patch_nums\n",
    "\n",
    "        if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':\n",
    "            self.output_projection = FlattenHead(configs.enc_in, self.head_nf, self.pred_len,\n",
    "                                                 head_dropout=configs.dropout)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.normalize_layers = Normalize(configs.enc_in, affine=False)\n",
    "\n",
    "    #def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n",
    "    def forward(self, x_enc, mask=None):\n",
    "        if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':\n",
    "            dec_out = self.forecast(x_enc)\n",
    "            return dec_out[:, -self.pred_len:, :]\n",
    "        return None\n",
    "\n",
    "    #def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "    def forecast(self, x_enc):\n",
    "        x_enc = self.normalize_layers(x_enc, 'norm')\n",
    "\n",
    "        B, T, N = x_enc.size()\n",
    "        x_enc = x_enc.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n",
    "\n",
    "        min_values = torch.min(x_enc, dim=1)[0]\n",
    "        max_values = torch.max(x_enc, dim=1)[0]\n",
    "        medians = torch.median(x_enc, dim=1).values\n",
    "        lags = self.calcute_lags(x_enc)\n",
    "        trends = x_enc.diff(dim=1).sum(dim=1)\n",
    "\n",
    "        prompt = []\n",
    "        for b in range(x_enc.shape[0]):\n",
    "            min_values_str = str(min_values[b].tolist()[0])\n",
    "            max_values_str = str(max_values[b].tolist()[0])\n",
    "            median_values_str = str(medians[b].tolist()[0])\n",
    "            lags_values_str = str(lags[b].tolist())\n",
    "            prompt_ = (\n",
    "                f\"<|start_prompt|>Dataset description: {self.description}\"\n",
    "                f\"Task description: forecast the next {str(self.pred_len)} steps given the previous {str(self.seq_len)} steps information; \"\n",
    "                \"Input statistics: \"\n",
    "                f\"min value {min_values_str}, \"\n",
    "                f\"max value {max_values_str}, \"\n",
    "                f\"median value {median_values_str}, \"\n",
    "                f\"the trend of input is {'upward' if trends[b] > 0 else 'downward'}, \"\n",
    "                f\"top 5 lags are : {lags_values_str}<|<end_prompt>|>\"\n",
    "            )\n",
    "\n",
    "            prompt.append(prompt_)\n",
    "\n",
    "        x_enc = x_enc.reshape(B, N, T).permute(0, 2, 1).contiguous()\n",
    "\n",
    "        prompt = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048).input_ids\n",
    "        prompt_embeddings = self.llm_model.get_input_embeddings()(prompt.to(x_enc.device))  # (batch, prompt_token, dim)\n",
    "\n",
    "        source_embeddings = self.mapping_layer(self.word_embeddings.permute(1, 0)).permute(1, 0)\n",
    "\n",
    "        x_enc = x_enc.permute(0, 2, 1).contiguous()\n",
    "        enc_out, n_vars = self.patch_embedding(x_enc)#.to(torch.bfloat16))\n",
    "        enc_out = self.reprogramming_layer(enc_out, source_embeddings, source_embeddings)\n",
    "        llama_enc_out = torch.cat([prompt_embeddings, enc_out], dim=1)\n",
    "        dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state\n",
    "        dec_out = dec_out[:, :, :self.d_ff]\n",
    "\n",
    "        dec_out = torch.reshape(\n",
    "            dec_out, (-1, n_vars, dec_out.shape[-2], dec_out.shape[-1]))\n",
    "        dec_out = dec_out.permute(0, 1, 3, 2).contiguous()\n",
    "\n",
    "        dec_out = self.output_projection(dec_out[:, :, :, -self.patch_nums:])\n",
    "        dec_out = dec_out.permute(0, 2, 1).contiguous()\n",
    "\n",
    "        dec_out = self.normalize_layers(dec_out, 'denorm')\n",
    "\n",
    "        return dec_out\n",
    "\n",
    "    def calcute_lags(self, x_enc):\n",
    "        q_fft = torch.fft.rfft(x_enc.permute(0, 2, 1).contiguous(), dim=-1)\n",
    "        k_fft = torch.fft.rfft(x_enc.permute(0, 2, 1).contiguous(), dim=-1)\n",
    "        res = q_fft * torch.conj(k_fft)\n",
    "        corr = torch.fft.irfft(res, dim=-1)\n",
    "        mean_value = torch.mean(corr, dim=1)\n",
    "        _, lags = torch.topk(mean_value, self.top_k, dim=-1)\n",
    "        return lags\n",
    "\n",
    "\n",
    "class ReprogrammingLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_keys=None, d_llm=None, attention_dropout=0.1):\n",
    "        super(ReprogrammingLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_llm, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_llm, d_keys * n_heads)\n",
    "        self.out_projection = nn.Linear(d_keys * n_heads, d_llm)\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def forward(self, target_embedding, source_embedding, value_embedding):\n",
    "        B, L, _ = target_embedding.shape\n",
    "        S, _ = source_embedding.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        target_embedding = self.query_projection(target_embedding).view(B, L, H, -1)\n",
    "        source_embedding = self.key_projection(source_embedding).view(S, H, -1)\n",
    "        value_embedding = self.value_projection(value_embedding).view(S, H, -1)\n",
    "\n",
    "        out = self.reprogramming(target_embedding, source_embedding, value_embedding)\n",
    "\n",
    "        out = out.reshape(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out)\n",
    "\n",
    "    def reprogramming(self, target_embedding, source_embedding, value_embedding):\n",
    "        B, L, H, E = target_embedding.shape\n",
    "\n",
    "        scale = 1. / sqrt(E)\n",
    "\n",
    "        scores = torch.einsum(\"blhe,she->bhls\", target_embedding, source_embedding)\n",
    "\n",
    "        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n",
    "        reprogramming_embedding = torch.einsum(\"bhls,she->blhe\", A, value_embedding)\n",
    "\n",
    "        return reprogramming_embedding\n",
    "\n",
    "\n",
    "def vali(args, model, vali_data, vali_loader, criterion, mae_metric, vis_flag):\n",
    "    total_loss = []\n",
    "    total_mae_loss = []\n",
    "    model.eval()\n",
    "    op = []\n",
    "    pr = []\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_x, batch_y) in tqdm(enumerate(vali_loader)):\n",
    "            batch_x = batch_x.float()\n",
    "            batch_y = batch_y.float()\n",
    "\n",
    "            #batch_x_mark = batch_x_mark.float().to(accelerator.device)\n",
    "            #batch_y_mark = batch_y_mark.float().to(accelerator.device)\n",
    "\n",
    "            # decoder input\n",
    "            dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float()\n",
    "            dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float()\n",
    "            # encoder - decoder\n",
    "\n",
    "            if args.output_attention:\n",
    "                outputs = model(batch_x, dec_inp)[0]\n",
    "            else:\n",
    "                outputs = model(batch_x, dec_inp)\n",
    "\n",
    "            #outputs, batch_y = accelerator.gather_for_metrics((outputs, batch_y))\n",
    "\n",
    "            f_dim = -1 if args.features == 'MS' else 0\n",
    "            outputs = outputs[:, -args.pred_len:, f_dim:]\n",
    "            batch_y = batch_y[:, -args.pred_len:, f_dim:]\n",
    "\n",
    "            pred = outputs.detach()\n",
    "            true = batch_y.detach()\n",
    "            #print(pred.shape)\n",
    "        \n",
    "\n",
    "            loss = criterion(pred, true)\n",
    "\n",
    "            mae_loss = mae_metric(pred, true)\n",
    "\n",
    "            total_loss.append(loss.item())\n",
    "            total_mae_loss.append(mae_loss.item())\n",
    "\n",
    "            if vis_flag:\n",
    "                o1 = true[0][0]\n",
    "                p1 = pred[0][0]\n",
    "                for i in range(1,len(true)):\n",
    "                    o1 = torch.cat((o1,true[i][0][-1].unsqueeze(0)))\n",
    "                    p1 = torch.cat((p1,pred[i][0][-1].unsqueeze(0)))\n",
    "                #op.append(vali_data.scaler.inverse_transform(o1.reshape(1,-1).cpu()))\n",
    "                #pr.append(vali_data.scaler.inverse_transform(p1.reshape(1,-1).cpu())) \n",
    "                op.append(o1.reshape(1,-1).cpu())\n",
    "                pr.append(p1.reshape(1,-1).cpu())  \n",
    "\n",
    "    total_loss = np.average(total_loss)\n",
    "    total_mae_loss = np.average(total_mae_loss)\n",
    "\n",
    "    model.train()\n",
    "    return total_loss, total_mae_loss, op, pr\n",
    "\n",
    "def del_files(dir_path):\n",
    "    shutil.rmtree(dir_path)\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, scheduler, epoch, args, printout=True):\n",
    "    if args.lradj == 'type1':\n",
    "        lr_adjust = {epoch: args.learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
    "    elif args.lradj == 'type2':\n",
    "        lr_adjust = {\n",
    "            2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "            10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "        }\n",
    "    elif args.lradj == 'type3':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 3 else args.learning_rate * (0.9 ** ((epoch - 3) // 1))}\n",
    "    elif args.lradj == 'PEMS':\n",
    "        lr_adjust = {epoch: args.learning_rate * (0.95 ** (epoch // 1))}\n",
    "    elif args.lradj == 'TST':\n",
    "        lr_adjust = {epoch: scheduler.get_last_lr()[0]}\n",
    "    elif args.lradj == 'constant':\n",
    "        lr_adjust = {epoch: args.learning_rate}\n",
    "    if epoch in lr_adjust.keys():\n",
    "        lr = lr_adjust[epoch]\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        if printout:\n",
    "            print('Updating learning rate to {}'.format(lr))\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, save_mode=True):\n",
    "        #self.accelerator = accelerator\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.save_mode = save_mode\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            if self.save_mode:\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            if self.save_mode:\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "\n",
    "\n",
    "        torch.save(model.state_dict(), path + '/' + 'checkpoint')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n",
    "\n",
    "#from utils.tools import del_files, EarlyStopping, adjust_learning_rate, vali, load_content\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Time-LLM')\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'task_name': 'short_term_forecast',\n",
    "    'is_training': 1,\n",
    "    'model_id': 'test',\n",
    "    'model_comment': 'none',\n",
    "    'seed': 2021,\n",
    "    'data': 'agri',\n",
    "    'root_path': './',\n",
    "    'data_path': 'raw_dat_v2.csv',\n",
    "    'features': 'S',\n",
    "    'target': 'RH1(%)',\n",
    "    'loader': 'modal',\n",
    "    'checkpoints': './checkpoints/',\n",
    "    'seq_len': 48,\n",
    "    'label_len': 24,\n",
    "    'pred_len': 10,\n",
    "    'enc_in': 7,\n",
    "    'dec_in': 7,\n",
    "    'c_out': 7,\n",
    "    'd_model': 16,\n",
    "    'n_heads': 8,\n",
    "    'e_layers': 2,\n",
    "    'd_layers': 1,\n",
    "    'd_ff': 32,\n",
    "    'moving_avg': 25,\n",
    "    'factor': 1,\n",
    "    'dropout': 0.1,\n",
    "    'activation': 'gelu',\n",
    "    'output_attention': False,\n",
    "    'patch_len': 16,\n",
    "    'stride': 8,\n",
    "    'prompt_domain': 0,\n",
    "    'llm_model': 'BERT',\n",
    "    'llm_dim': 768, #4096 for Llama\n",
    "    'num_workers': 10,\n",
    "    'itr': 1,\n",
    "    'train_epochs': 20,\n",
    "    'align_epochs': 1,\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 1,\n",
    "    'patience': 10,\n",
    "    'learning_rate': 0.001,\n",
    "    'des': 'test',\n",
    "    'loss': 'MSE',\n",
    "    'lradj': 'type1',\n",
    "    'pct_start': 0.2,\n",
    "    'use_amp': False,\n",
    "    'llm_layers': 1,\n",
    "    'percent': 100\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#args = parser.parse_args()\n",
    "args = argparse.Namespace(**default_args)\n",
    "\n",
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_{}_{}'.format(\n",
    "        args.task_name,\n",
    "        args.model_id,\n",
    "        args.data,\n",
    "        args.features,\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.factor,\n",
    "        #args.embed,\n",
    "        args.des, ii)\n",
    "\n",
    "    train_data, train_loader = data_provider(args, 'train')\n",
    "    vali_data, vali_loader = data_provider(args, 'val')\n",
    "    test_data, test_loader = data_provider(args, 'test')\n",
    "\n",
    "    '''if args.model == 'Autoformer':\n",
    "        model = Autoformer.Model(args).float()\n",
    "    elif args.model == 'DLinear':\n",
    "        model = DLinear.Model(args).float()\n",
    "    else:'''\n",
    "\n",
    "\n",
    "    model = Model(args).float().to(device)\n",
    "\n",
    "    train_loader = [(data.to(device), target.to(device)) for data, target in train_loader]\n",
    "    test_loader = [(data.to(device), target.to(device)) for data, target in test_loader]\n",
    "    vali_loader = [(data.to(device), target.to(device)) for data, target in vali_loader]\n",
    "\n",
    "    path = os.path.join(args.checkpoints,\n",
    "                        setting + '-' + args.model_comment)  # unique checkpoint saving path\n",
    "    #args.content = load_content(args)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    time_now = time.time()\n",
    "\n",
    "    train_steps = len(train_loader)\n",
    "    early_stopping = EarlyStopping(patience=args.patience)\n",
    "\n",
    "    trained_parameters = []\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad is True:\n",
    "            trained_parameters.append(p)\n",
    "\n",
    "    model_optim = optim.Adam(trained_parameters, lr=args.learning_rate)\n",
    "\n",
    "    if args.lradj == 'COS':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(model_optim, T_max=20, eta_min=1e-8)\n",
    "    else:\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer=model_optim,\n",
    "                                            steps_per_epoch=train_steps,\n",
    "                                            pct_start=args.pct_start,\n",
    "                                            epochs=args.train_epochs,\n",
    "                                            max_lr=args.learning_rate)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    mae_metric = nn.L1Loss()\n",
    "    ops_rh = []\n",
    "    prs_rh = []\n",
    "    t_loss_rh = []\n",
    "    v_loss_rh = []\n",
    "    for epoch in range(args.train_epochs):\n",
    "        iter_count = 0\n",
    "        train_loss = []\n",
    "\n",
    "        model.train()\n",
    "        epoch_time = time.time()\n",
    "        for i, (batch_x, batch_y) in tqdm(enumerate(train_loader)):\n",
    "            iter_count += 1\n",
    "            model_optim.zero_grad()\n",
    "\n",
    "            batch_x = batch_x.float()\n",
    "            batch_y = batch_y.float()\n",
    "            #batch_x_mark = batch_x_mark.float()\n",
    "            #batch_y_mark = batch_y_mark.float()\n",
    "\n",
    "            dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float()\n",
    "            dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float()\n",
    "\n",
    "            if args.output_attention:\n",
    "                outputs = model(batch_x, dec_inp)[0]\n",
    "            else:\n",
    "                outputs = model(batch_x, dec_inp)\n",
    "\n",
    "\n",
    "            f_dim = -1 if args.features == 'MS' else 0\n",
    "            outputs = outputs[:, -args.pred_len:, f_dim:]\n",
    "            batch_y = batch_y[:, -args.pred_len:, f_dim:]\n",
    "\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            model_optim.step()\n",
    "\n",
    "            if args.lradj == 'TST':\n",
    "                adjust_learning_rate(model_optim, scheduler, epoch + 1, args)\n",
    "\n",
    "        print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "        train_loss = np.average(train_loss)\n",
    "        t_loss_rh.append(train_loss)\n",
    "\n",
    "        vali_loss, vali_mae_loss,_,_ = vali(args, model, vali_data, vali_loader, criterion, mae_metric, vis_flag=False)\n",
    "        v_loss_rh.append(vali_loss)\n",
    "\n",
    "        if epoch == (args.train_epochs-1):\n",
    "            vis_flag = True\n",
    "            test_loss, test_mae_loss, ops_rh, prs_rh = vali(args, model, test_data, test_loader, criterion, mae_metric, vis_flag)\n",
    "            f_loss_rh = (test_loss, test_mae_loss)\n",
    "        else:\n",
    "            vis_flag = False\n",
    "            test_loss, test_mae_loss, _, _ = vali(args, model, test_data, test_loader, criterion, mae_metric, vis_flag)\n",
    "        \n",
    "\n",
    "        \n",
    "        print(\n",
    "            \"Epoch: {0} | Train Loss: {1:.7f} Vali Loss: {2:.7f} Test Loss: {3:.7f} MAE Loss: {4:.7f}\".format(\n",
    "                epoch + 1, train_loss, vali_loss, test_loss, test_mae_loss))\n",
    "\n",
    "        early_stopping(vali_loss, model, path)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        if args.lradj != 'TST':\n",
    "            if args.lradj == 'COS':\n",
    "                scheduler.step()\n",
    "                print(\"lr = {:.10f}\".format(model_optim.param_groups[0]['lr']))\n",
    "            else:\n",
    "                if epoch == 0:\n",
    "                    args.learning_rate = model_optim.param_groups[0]['lr']\n",
    "                    print(\"lr = {:.10f}\".format(model_optim.param_groups[0]['lr']))\n",
    "                adjust_learning_rate(model_optim, scheduler, epoch + 1, args, printout=True)\n",
    "\n",
    "        else:\n",
    "            print('Updating learning rate to {}'.format(scheduler.get_last_lr()[0]))\n",
    "\n",
    "if not os.path.exists('./checkpoints'):\n",
    "    os.makedirs('./checkpoints')  # unique checkpoint saving path\n",
    "del_files('./checkpoints')  # delete checkpoint files\n",
    "print('success delete checkpoints')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "o1_rh = ops_rh[0][0]\n",
    "p1_rh = prs_rh[0][0]\n",
    "for i in range(1,len(ops_rh)):\n",
    "    o1_rh = np.concatenate((o1_rh,ops_rh[1][0]))\n",
    "    p1_rh = np.concatenate((p1_rh,prs_rh[1][0]))\n",
    "\n",
    "o2_rh = test_data.scaler.inverse_transform(o1_rh.reshape(-1,1))\n",
    "p2_rh = test_data.scaler.inverse_transform(p1_rh.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig= plt.figure()\n",
    "\n",
    "# Plot o1\n",
    "plt.plot(o2_rh, label='truth')\n",
    "\n",
    "# Plot p1\n",
    "plt.plot(p2_rh, label='pred')\n",
    "\n",
    "# Labeling and title\n",
    "plt.xlabel('Week Index')\n",
    "plt.ylabel('RH(%)')\n",
    "plt.title('RH(%) Truth vs Prediction')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot as an image (e.g., PNG)\n",
    "plt.savefig('plot_image_rh_20_epoch.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost time: 2.465379476547241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.48it/s]\n",
      "7it [00:00, 17.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Train Loss: 1.1403683 Vali Loss: 0.8128854 Test Loss: 0.7543623 MAE Loss: 0.7133577\n",
      "lr = 0.0000400000\n",
      "Updating learning rate to 3.9999999999999996e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 cost time: 2.461367130279541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.31it/s]\n",
      "7it [00:00, 17.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Train Loss: 0.5931101 Vali Loss: 0.3852404 Test Loss: 0.3627107 MAE Loss: 0.4772310\n",
      "Updating learning rate to 1.9999999999999998e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 cost time: 2.4503564834594727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.56it/s]\n",
      "7it [00:00, 17.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Train Loss: 0.4354171 Vali Loss: 0.3470603 Test Loss: 0.3160351 MAE Loss: 0.4416593\n",
      "Updating learning rate to 9.999999999999999e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 cost time: 2.4409279823303223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.26it/s]\n",
      "7it [00:00, 17.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Train Loss: 0.4020553 Vali Loss: 0.3295444 Test Loss: 0.3024368 MAE Loss: 0.4313449\n",
      "Updating learning rate to 4.9999999999999996e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 cost time: 2.459818124771118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.29it/s]\n",
      "7it [00:00, 17.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Train Loss: 0.3815955 Vali Loss: 0.3224961 Test Loss: 0.2965999 MAE Loss: 0.4267493\n",
      "Updating learning rate to 2.4999999999999998e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 cost time: 2.459808588027954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.34it/s]\n",
      "7it [00:00, 17.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Train Loss: 0.3890821 Vali Loss: 0.3199001 Test Loss: 0.2943950 MAE Loss: 0.4245915\n",
      "Updating learning rate to 1.2499999999999999e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 cost time: 2.461016893386841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.35it/s]\n",
      "7it [00:00, 17.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Train Loss: 0.3827361 Vali Loss: 0.3187129 Test Loss: 0.2932909 MAE Loss: 0.4236978\n",
      "Updating learning rate to 6.249999999999999e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 cost time: 2.4288618564605713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.24it/s]\n",
      "7it [00:00, 17.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Train Loss: 0.3846665 Vali Loss: 0.3182466 Test Loss: 0.2928309 MAE Loss: 0.4233288\n",
      "Updating learning rate to 3.1249999999999997e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 cost time: 2.4700767993927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.51it/s]\n",
      "7it [00:00, 17.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Train Loss: 0.3854125 Vali Loss: 0.3179863 Test Loss: 0.2926104 MAE Loss: 0.4231448\n",
      "Updating learning rate to 1.5624999999999999e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 cost time: 2.458207130432129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.47it/s]\n",
      "7it [00:00, 17.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Train Loss: 0.3841495 Vali Loss: 0.3178361 Test Loss: 0.2925163 MAE Loss: 0.4230426\n",
      "Updating learning rate to 7.812499999999999e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 cost time: 2.4500508308410645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.58it/s]\n",
      "7it [00:00, 17.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Train Loss: 0.3801501 Vali Loss: 0.3177690 Test Loss: 0.2924558 MAE Loss: 0.4229894\n",
      "Updating learning rate to 3.9062499999999997e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 cost time: 2.461225748062134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.57it/s]\n",
      "7it [00:00, 17.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Train Loss: 0.3836933 Vali Loss: 0.3177394 Test Loss: 0.2924209 MAE Loss: 0.4229635\n",
      "Updating learning rate to 1.9531249999999998e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 cost time: 2.4612479209899902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.28it/s]\n",
      "7it [00:00, 17.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Train Loss: 0.3881044 Vali Loss: 0.3177226 Test Loss: 0.2924041 MAE Loss: 0.4229525\n",
      "Updating learning rate to 9.765624999999999e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 cost time: 2.4625842571258545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.44it/s]\n",
      "7it [00:00, 17.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Train Loss: 0.3847997 Vali Loss: 0.3177128 Test Loss: 0.2923970 MAE Loss: 0.4229464\n",
      "Updating learning rate to 4.8828124999999996e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 cost time: 2.4552979469299316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.40it/s]\n",
      "7it [00:00, 17.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Train Loss: 0.3743803 Vali Loss: 0.3177084 Test Loss: 0.2923940 MAE Loss: 0.4229437\n",
      "Updating learning rate to 2.4414062499999998e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 cost time: 2.471978187561035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.42it/s]\n",
      "7it [00:00, 17.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Train Loss: 0.3806596 Vali Loss: 0.3177068 Test Loss: 0.2923929 MAE Loss: 0.4229427\n",
      "Updating learning rate to 1.2207031249999999e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 cost time: 2.451937437057495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.38it/s]\n",
      "7it [00:00, 17.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Train Loss: 0.3801715 Vali Loss: 0.3177064 Test Loss: 0.2923926 MAE Loss: 0.4229424\n",
      "Updating learning rate to 6.103515624999999e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 cost time: 2.451754093170166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.26it/s]\n",
      "7it [00:00, 17.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Train Loss: 0.3949558 Vali Loss: 0.3177063 Test Loss: 0.2923925 MAE Loss: 0.4229424\n",
      "Updating learning rate to 3.0517578124999997e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 cost time: 2.4707016944885254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.45it/s]\n",
      "7it [00:00, 17.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Train Loss: 0.3890555 Vali Loss: 0.3177062 Test Loss: 0.2923925 MAE Loss: 0.4229423\n",
      "Updating learning rate to 1.5258789062499999e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 cost time: 2.46701979637146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.53it/s]\n",
      "7it [00:00, 17.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Train Loss: 0.3906893 Vali Loss: 0.3177062 Test Loss: 0.2923925 MAE Loss: 0.4229423\n",
      "Updating learning rate to 7.629394531249999e-11\n",
      "success delete checkpoints\n"
     ]
    }
   ],
   "source": [
    "default_args = {\n",
    "    'task_name': 'short_term_forecast',\n",
    "    'is_training': 1,\n",
    "    'model_id': 'test',\n",
    "    'model_comment': 'none',\n",
    "    'seed': 2021,\n",
    "    'data': 'agri',\n",
    "    'root_path': './',\n",
    "    'data_path': 'raw_dat_v2.csv',\n",
    "    'features': 'S',\n",
    "    'target': 'MaxT(°C)',\n",
    "    'loader': 'modal',\n",
    "    'checkpoints': './checkpoints/',\n",
    "    'seq_len': 48,\n",
    "    'label_len': 24,\n",
    "    'pred_len': 10,\n",
    "    'enc_in': 7,\n",
    "    'dec_in': 7,\n",
    "    'c_out': 7,\n",
    "    'd_model': 16,\n",
    "    'n_heads': 8,\n",
    "    'e_layers': 2,\n",
    "    'd_layers': 1,\n",
    "    'd_ff': 32,\n",
    "    'moving_avg': 25,\n",
    "    'factor': 1,\n",
    "    'dropout': 0.1,\n",
    "    'activation': 'gelu',\n",
    "    'output_attention': False,\n",
    "    'patch_len': 16,\n",
    "    'stride': 8,\n",
    "    'prompt_domain': 0,\n",
    "    'llm_model': 'BERT',\n",
    "    'llm_dim': 768, #4096 for Llama\n",
    "    'num_workers': 10,\n",
    "    'itr': 1,\n",
    "    'train_epochs': 20,\n",
    "    'align_epochs': 1,\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 1,\n",
    "    'patience': 10,\n",
    "    'learning_rate': 0.001,\n",
    "    'des': 'test',\n",
    "    'loss': 'MSE',\n",
    "    'lradj': 'type1',\n",
    "    'pct_start': 0.2,\n",
    "    'use_amp': False,\n",
    "    'llm_layers': 1,\n",
    "    'percent': 100\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#args = parser.parse_args()\n",
    "args = argparse.Namespace(**default_args)\n",
    "\n",
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_{}_{}'.format(\n",
    "        args.task_name,\n",
    "        args.model_id,\n",
    "        args.data,\n",
    "        args.features,\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.factor,\n",
    "        #args.embed,\n",
    "        args.des, ii)\n",
    "\n",
    "    train_data, train_loader = data_provider(args, 'train')\n",
    "    vali_data, vali_loader = data_provider(args, 'val')\n",
    "    test_data, test_loader = data_provider(args, 'test')\n",
    "\n",
    "    '''if args.model == 'Autoformer':\n",
    "        model = Autoformer.Model(args).float()\n",
    "    elif args.model == 'DLinear':\n",
    "        model = DLinear.Model(args).float()\n",
    "    else:'''\n",
    "\n",
    "\n",
    "    model = Model(args).float().to(device)\n",
    "\n",
    "    train_loader = [(data.to(device), target.to(device)) for data, target in train_loader]\n",
    "    test_loader = [(data.to(device), target.to(device)) for data, target in test_loader]\n",
    "    vali_loader = [(data.to(device), target.to(device)) for data, target in vali_loader]\n",
    "\n",
    "    path = os.path.join(args.checkpoints,\n",
    "                        setting + '-' + args.model_comment)  # unique checkpoint saving path\n",
    "    #args.content = load_content(args)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    time_now = time.time()\n",
    "\n",
    "    train_steps = len(train_loader)\n",
    "    early_stopping = EarlyStopping(patience=args.patience)\n",
    "\n",
    "    trained_parameters = []\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad is True:\n",
    "            trained_parameters.append(p)\n",
    "\n",
    "    model_optim = optim.Adam(trained_parameters, lr=args.learning_rate)\n",
    "\n",
    "    if args.lradj == 'COS':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(model_optim, T_max=20, eta_min=1e-8)\n",
    "    else:\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer=model_optim,\n",
    "                                            steps_per_epoch=train_steps,\n",
    "                                            pct_start=args.pct_start,\n",
    "                                            epochs=args.train_epochs,\n",
    "                                            max_lr=args.learning_rate)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    mae_metric = nn.L1Loss()\n",
    "    ops_maxt = []\n",
    "    prs_maxt = []\n",
    "    t_loss_maxt = []\n",
    "    v_loss_maxt = []\n",
    "    for epoch in range(args.train_epochs):\n",
    "        iter_count = 0\n",
    "        train_loss = []\n",
    "\n",
    "        model.train()\n",
    "        epoch_time = time.time()\n",
    "        for i, (batch_x, batch_y) in tqdm(enumerate(train_loader)):\n",
    "            iter_count += 1\n",
    "            model_optim.zero_grad()\n",
    "\n",
    "            batch_x = batch_x.float()\n",
    "            batch_y = batch_y.float()\n",
    "            #batch_x_mark = batch_x_mark.float()\n",
    "            #batch_y_mark = batch_y_mark.float()\n",
    "\n",
    "            dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float()\n",
    "            dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float()\n",
    "\n",
    "            if args.output_attention:\n",
    "                outputs = model(batch_x, dec_inp)[0]\n",
    "            else:\n",
    "                outputs = model(batch_x, dec_inp)\n",
    "\n",
    "\n",
    "            f_dim = -1 if args.features == 'MS' else 0\n",
    "            outputs = outputs[:, -args.pred_len:, f_dim:]\n",
    "            batch_y = batch_y[:, -args.pred_len:, f_dim:]\n",
    "\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            model_optim.step()\n",
    "\n",
    "            if args.lradj == 'TST':\n",
    "                adjust_learning_rate(model_optim, scheduler, epoch + 1, args)\n",
    "\n",
    "        print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "        train_loss = np.average(train_loss)\n",
    "        t_loss_maxt.append(train_loss)\n",
    "\n",
    "        vali_loss, vali_mae_loss,_,_ = vali(args, model, vali_data, vali_loader, criterion, mae_metric, vis_flag=False)\n",
    "        v_loss_maxt.append(vali_loss)\n",
    "        if epoch == (args.train_epochs-1):\n",
    "            vis_flag = True\n",
    "            test_loss, test_mae_loss, ops_maxt, prs_maxt = vali(args, model, test_data, test_loader, criterion, mae_metric, vis_flag)\n",
    "            f_loss_maxt = (test_loss, test_mae_loss)\n",
    "        else:\n",
    "            vis_flag = False\n",
    "            test_loss, test_mae_loss, _, _ = vali(args, model, test_data, test_loader, criterion, mae_metric, vis_flag)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        print(\n",
    "            \"Epoch: {0} | Train Loss: {1:.7f} Vali Loss: {2:.7f} Test Loss: {3:.7f} MAE Loss: {4:.7f}\".format(\n",
    "                epoch + 1, train_loss, vali_loss, test_loss, test_mae_loss))\n",
    "\n",
    "        early_stopping(vali_loss, model, path)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        if args.lradj != 'TST':\n",
    "            if args.lradj == 'COS':\n",
    "                scheduler.step()\n",
    "                print(\"lr = {:.10f}\".format(model_optim.param_groups[0]['lr']))\n",
    "            else:\n",
    "                if epoch == 0:\n",
    "                    args.learning_rate = model_optim.param_groups[0]['lr']\n",
    "                    print(\"lr = {:.10f}\".format(model_optim.param_groups[0]['lr']))\n",
    "                adjust_learning_rate(model_optim, scheduler, epoch + 1, args, printout=True)\n",
    "\n",
    "        else:\n",
    "            print('Updating learning rate to {}'.format(scheduler.get_last_lr()[0]))\n",
    "\n",
    "if not os.path.exists('./checkpoints'):\n",
    "    os.makedirs('./checkpoints')  # unique checkpoint saving path\n",
    "del_files('./checkpoints')  # delete checkpoint files\n",
    "print('success delete checkpoints')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "o1_maxt = ops_maxt[0][0]\n",
    "p1_maxt = prs_maxt[0][0]\n",
    "for i in range(1,len(ops_maxt)):\n",
    "    o1_maxt = np.concatenate((o1_maxt,ops_maxt[1][0]))\n",
    "    p1_maxt = np.concatenate((p1_maxt,prs_maxt[1][0]))\n",
    "\n",
    "o2_maxt = test_data.scaler.inverse_transform(o1_maxt.reshape(-1,1))\n",
    "p2_maxt = test_data.scaler.inverse_transform(p1_maxt.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig= plt.figure()\n",
    "\n",
    "# Plot o1\n",
    "plt.plot(o2_maxt, label='Truth')\n",
    "\n",
    "# Plot p1\n",
    "plt.plot(p2_maxt, label='Pred')\n",
    "\n",
    "# Labeling and title\n",
    "plt.xlabel('Week Index')\n",
    "plt.ylabel('MaxT(°C)')\n",
    "plt.title('MaxT(°C) Truth vs Prediction')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot as an image (e.g., PNG)\n",
    "plt.savefig('plot_image_maxT_20_epoch.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost time: 2.474858522415161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.63it/s]\n",
      "7it [00:00, 17.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Train Loss: 1.0829811 Vali Loss: 2.0068264 Test Loss: 1.2037178 MAE Loss: 0.8205271\n",
      "lr = 0.0000400000\n",
      "Updating learning rate to 3.9999999999999996e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 cost time: 2.4664413928985596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.34it/s]\n",
      "7it [00:00, 17.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Train Loss: 1.0361135 Vali Loss: 2.0017895 Test Loss: 1.0943969 MAE Loss: 0.7260533\n",
      "Updating learning rate to 1.9999999999999998e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 cost time: 2.457048177719116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.39it/s]\n",
      "7it [00:00, 17.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Train Loss: 0.9855538 Vali Loss: 1.9124204 Test Loss: 1.0468442 MAE Loss: 0.7298978\n",
      "Updating learning rate to 9.999999999999999e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 cost time: 2.454230308532715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.33it/s]\n",
      "7it [00:00, 17.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Train Loss: 0.9520907 Vali Loss: 1.8899674 Test Loss: 1.0032257 MAE Loss: 0.6901070\n",
      "Updating learning rate to 4.9999999999999996e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 cost time: 2.4649770259857178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.36it/s]\n",
      "7it [00:00, 17.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Train Loss: 0.9536541 Vali Loss: 1.8798970 Test Loss: 0.9915114 MAE Loss: 0.6864381\n",
      "Updating learning rate to 2.4999999999999998e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 cost time: 2.46930193901062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.33it/s]\n",
      "7it [00:00, 17.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Train Loss: 0.9341200 Vali Loss: 1.8738878 Test Loss: 0.9866676 MAE Loss: 0.6861999\n",
      "Updating learning rate to 1.2499999999999999e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 cost time: 2.4543981552124023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.40it/s]\n",
      "7it [00:00, 17.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Train Loss: 0.9330187 Vali Loss: 1.8705339 Test Loss: 0.9832016 MAE Loss: 0.6842328\n",
      "Updating learning rate to 6.249999999999999e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 cost time: 2.4529268741607666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.43it/s]\n",
      "7it [00:00, 17.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Train Loss: 0.9308246 Vali Loss: 1.8689636 Test Loss: 0.9816290 MAE Loss: 0.6832650\n",
      "Updating learning rate to 3.1249999999999997e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 cost time: 2.4655959606170654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.30it/s]\n",
      "7it [00:00, 17.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Train Loss: 0.9312532 Vali Loss: 1.8683337 Test Loss: 0.9808271 MAE Loss: 0.6826293\n",
      "Updating learning rate to 1.5624999999999999e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 cost time: 2.469823122024536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.38it/s]\n",
      "7it [00:00, 17.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Train Loss: 0.9346902 Vali Loss: 1.8680889 Test Loss: 0.9804694 MAE Loss: 0.6823593\n",
      "Updating learning rate to 7.812499999999999e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 cost time: 2.4670815467834473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.22it/s]\n",
      "7it [00:00, 17.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Train Loss: 0.9331522 Vali Loss: 1.8679654 Test Loss: 0.9802959 MAE Loss: 0.6822404\n",
      "Updating learning rate to 3.9062499999999997e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 cost time: 2.4657208919525146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.30it/s]\n",
      "7it [00:00, 17.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Train Loss: 0.9268735 Vali Loss: 1.8678947 Test Loss: 0.9802019 MAE Loss: 0.6821747\n",
      "Updating learning rate to 1.9531249999999998e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 cost time: 2.464167594909668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.32it/s]\n",
      "7it [00:00, 17.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Train Loss: 0.9339287 Vali Loss: 1.8678614 Test Loss: 0.9801537 MAE Loss: 0.6821409\n",
      "Updating learning rate to 9.765624999999999e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 cost time: 2.469905376434326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.35it/s]\n",
      "7it [00:00, 17.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Train Loss: 0.9340230 Vali Loss: 1.8678381 Test Loss: 0.9801300 MAE Loss: 0.6821269\n",
      "Updating learning rate to 4.8828124999999996e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 cost time: 2.4678826332092285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.33it/s]\n",
      "7it [00:00, 17.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Train Loss: 0.9318044 Vali Loss: 1.8678294 Test Loss: 0.9801193 MAE Loss: 0.6821197\n",
      "Updating learning rate to 2.4414062499999998e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 cost time: 2.4531161785125732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.27it/s]\n",
      "7it [00:00, 17.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Train Loss: 0.9317101 Vali Loss: 1.8678262 Test Loss: 0.9801150 MAE Loss: 0.6821161\n",
      "Updating learning rate to 1.2207031249999999e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 cost time: 2.4633774757385254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.30it/s]\n",
      "7it [00:00, 17.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Train Loss: 0.9383535 Vali Loss: 1.8678254 Test Loss: 0.9801136 MAE Loss: 0.6821150\n",
      "Updating learning rate to 6.103515624999999e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 cost time: 2.45939564704895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.26it/s]\n",
      "7it [00:00, 17.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Train Loss: 0.9375523 Vali Loss: 1.8678251 Test Loss: 0.9801133 MAE Loss: 0.6821147\n",
      "Updating learning rate to 3.0517578124999997e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 cost time: 2.4564406871795654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.37it/s]\n",
      "7it [00:00, 17.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Train Loss: 0.9339080 Vali Loss: 1.8678251 Test Loss: 0.9801132 MAE Loss: 0.6821146\n",
      "Updating learning rate to 1.5258789062499999e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 cost time: 2.4620745182037354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.38it/s]\n",
      "7it [00:00, 17.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Train Loss: 0.9268297 Vali Loss: 1.8678250 Test Loss: 0.9801132 MAE Loss: 0.6821146\n",
      "Updating learning rate to 7.629394531249999e-11\n",
      "success delete checkpoints\n"
     ]
    }
   ],
   "source": [
    "default_args = {\n",
    "    'task_name': 'short_term_forecast',\n",
    "    'is_training': 1,\n",
    "    'model_id': 'test',\n",
    "    'model_comment': 'none',\n",
    "    'seed': 2021,\n",
    "    'data': 'agri',\n",
    "    'root_path': './',\n",
    "    'data_path': 'raw_dat_v2.csv',\n",
    "    'features': 'S',\n",
    "    'target': 'RF(mm)',\n",
    "    'loader': 'modal',\n",
    "    'checkpoints': './checkpoints/',\n",
    "    'seq_len': 48,\n",
    "    'label_len': 24,\n",
    "    'pred_len': 10,\n",
    "    'enc_in': 7,\n",
    "    'dec_in': 7,\n",
    "    'c_out': 7,\n",
    "    'd_model': 16,\n",
    "    'n_heads': 8,\n",
    "    'e_layers': 2,\n",
    "    'd_layers': 1,\n",
    "    'd_ff': 32,\n",
    "    'moving_avg': 25,\n",
    "    'factor': 1,\n",
    "    'dropout': 0.1,\n",
    "    'activation': 'gelu',\n",
    "    'output_attention': False,\n",
    "    'patch_len': 16,\n",
    "    'stride': 8,\n",
    "    'prompt_domain': 0,\n",
    "    'llm_model': 'BERT',\n",
    "    'llm_dim': 768, #4096 for Llama\n",
    "    'num_workers': 10,\n",
    "    'itr': 1,\n",
    "    'train_epochs': 20,\n",
    "    'align_epochs': 1,\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 1,\n",
    "    'patience': 10,\n",
    "    'learning_rate': 0.001,\n",
    "    'des': 'test',\n",
    "    'loss': 'MSE',\n",
    "    'lradj': 'type1',\n",
    "    'pct_start': 0.2,\n",
    "    'use_amp': False,\n",
    "    'llm_layers': 1,\n",
    "    'percent': 100\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#args = parser.parse_args()\n",
    "args = argparse.Namespace(**default_args)\n",
    "\n",
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_{}_{}'.format(\n",
    "        args.task_name,\n",
    "        args.model_id,\n",
    "        args.data,\n",
    "        args.features,\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.factor,\n",
    "        #args.embed,\n",
    "        args.des, ii)\n",
    "\n",
    "    train_data, train_loader = data_provider(args, 'train')\n",
    "    vali_data, vali_loader = data_provider(args, 'val')\n",
    "    test_data, test_loader = data_provider(args, 'test')\n",
    "\n",
    "    '''if args.model == 'Autoformer':\n",
    "        model = Autoformer.Model(args).float()\n",
    "    elif args.model == 'DLinear':\n",
    "        model = DLinear.Model(args).float()\n",
    "    else:'''\n",
    "\n",
    "\n",
    "    model = Model(args).float().to(device)\n",
    "\n",
    "    train_loader = [(data.to(device), target.to(device)) for data, target in train_loader]\n",
    "    test_loader = [(data.to(device), target.to(device)) for data, target in test_loader]\n",
    "    vali_loader = [(data.to(device), target.to(device)) for data, target in vali_loader]\n",
    "\n",
    "    path = os.path.join(args.checkpoints,\n",
    "                        setting + '-' + args.model_comment)  # unique checkpoint saving path\n",
    "    #args.content = load_content(args)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    time_now = time.time()\n",
    "\n",
    "    train_steps = len(train_loader)\n",
    "    early_stopping = EarlyStopping(patience=args.patience)\n",
    "\n",
    "    trained_parameters = []\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad is True:\n",
    "            trained_parameters.append(p)\n",
    "\n",
    "    model_optim = optim.Adam(trained_parameters, lr=args.learning_rate)\n",
    "\n",
    "    if args.lradj == 'COS':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(model_optim, T_max=20, eta_min=1e-8)\n",
    "    else:\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer=model_optim,\n",
    "                                            steps_per_epoch=train_steps,\n",
    "                                            pct_start=args.pct_start,\n",
    "                                            epochs=args.train_epochs,\n",
    "                                            max_lr=args.learning_rate)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    mae_metric = nn.L1Loss()\n",
    "    ops_rf = []\n",
    "    prs_rf = []\n",
    "    t_loss_rf = []\n",
    "    v_loss_rf = []\n",
    "    for epoch in range(args.train_epochs):\n",
    "        iter_count = 0\n",
    "        train_loss = []\n",
    "\n",
    "        model.train()\n",
    "        epoch_time = time.time()\n",
    "        for i, (batch_x, batch_y) in tqdm(enumerate(train_loader)):\n",
    "            iter_count += 1\n",
    "            model_optim.zero_grad()\n",
    "\n",
    "            batch_x = batch_x.float()\n",
    "            batch_y = batch_y.float()\n",
    "            #batch_x_mark = batch_x_mark.float()\n",
    "            #batch_y_mark = batch_y_mark.float()\n",
    "\n",
    "            dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float()\n",
    "            dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float()\n",
    "\n",
    "            if args.output_attention:\n",
    "                outputs = model(batch_x, dec_inp)[0]\n",
    "            else:\n",
    "                outputs = model(batch_x, dec_inp)\n",
    "\n",
    "\n",
    "            f_dim = -1 if args.features == 'MS' else 0\n",
    "            outputs = outputs[:, -args.pred_len:, f_dim:]\n",
    "            batch_y = batch_y[:, -args.pred_len:, f_dim:]\n",
    "\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            model_optim.step()\n",
    "\n",
    "            if args.lradj == 'TST':\n",
    "                adjust_learning_rate(model_optim, scheduler, epoch + 1, args)\n",
    "\n",
    "        print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "        train_loss = np.average(train_loss)\n",
    "        t_loss_rf.append(train_loss)\n",
    "\n",
    "        vali_loss, vali_mae_loss,_,_ = vali(args, model, vali_data, vali_loader, criterion, mae_metric, vis_flag=False)\n",
    "        v_loss_rf.append(vali_loss)\n",
    "\n",
    "        if epoch == (args.train_epochs-1):\n",
    "            vis_flag = True\n",
    "            test_loss, test_mae_loss, ops_rf, prs_rf = vali(args, model, test_data, test_loader, criterion, mae_metric, vis_flag)\n",
    "            f_loss_rf = (test_loss, test_mae_loss)\n",
    "        else:\n",
    "            vis_flag = False\n",
    "            test_loss, test_mae_loss, _, _ = vali(args, model, test_data, test_loader, criterion, mae_metric, vis_flag)\n",
    "        \n",
    "\n",
    "        \n",
    "        print(\n",
    "            \"Epoch: {0} | Train Loss: {1:.7f} Vali Loss: {2:.7f} Test Loss: {3:.7f} MAE Loss: {4:.7f}\".format(\n",
    "                epoch + 1, train_loss, vali_loss, test_loss, test_mae_loss))\n",
    "\n",
    "        early_stopping(vali_loss, model, path)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        if args.lradj != 'TST':\n",
    "            if args.lradj == 'COS':\n",
    "                scheduler.step()\n",
    "                print(\"lr = {:.10f}\".format(model_optim.param_groups[0]['lr']))\n",
    "            else:\n",
    "                if epoch == 0:\n",
    "                    args.learning_rate = model_optim.param_groups[0]['lr']\n",
    "                    print(\"lr = {:.10f}\".format(model_optim.param_groups[0]['lr']))\n",
    "                adjust_learning_rate(model_optim, scheduler, epoch + 1, args, printout=True)\n",
    "\n",
    "        else:\n",
    "            print('Updating learning rate to {}'.format(scheduler.get_last_lr()[0]))\n",
    "\n",
    "if not os.path.exists('./checkpoints'):\n",
    "    os.makedirs('./checkpoints')  # unique checkpoint saving path\n",
    "del_files('./checkpoints')  # delete checkpoint files\n",
    "print('success delete checkpoints')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "o1_rf = ops_rf[0][0]\n",
    "p1_rf = prs_rf[0][0]\n",
    "for i in range(1,len(ops_rf)):\n",
    "    o1_rf = np.concatenate((o1_rf,ops_rf[1][0]))\n",
    "    p1_rf = np.concatenate((p1_rf,prs_rf[1][0]))\n",
    "\n",
    "o2_rf = test_data.scaler.inverse_transform(o1_rf.reshape(-1,1))\n",
    "p2_rf = test_data.scaler.inverse_transform(p1_rf.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig= plt.figure()\n",
    "\n",
    "# Plot o1\n",
    "plt.plot(o2_rf, label='Truth')\n",
    "\n",
    "# Plot p1\n",
    "plt.plot(p2_rf, label='Pred')\n",
    "\n",
    "# Labeling and title\n",
    "plt.xlabel('Week Index')\n",
    "plt.ylabel('RF(mm)')\n",
    "plt.title('RF(mm) Truth vs Prediction')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot as an image (e.g., PNG)\n",
    "plt.savefig('plot_image_rf_20_epoch.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost time: 2.4626152515411377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.36it/s]\n",
      "7it [00:00, 17.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Train Loss: 1.1619161 Vali Loss: 1.1446088 Test Loss: 1.2154221 MAE Loss: 0.8905139\n",
      "lr = 0.0000400000\n",
      "Updating learning rate to 3.9999999999999996e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 cost time: 2.4492998123168945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.35it/s]\n",
      "7it [00:00, 17.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Train Loss: 0.7417067 Vali Loss: 0.4005243 Test Loss: 0.4639716 MAE Loss: 0.5268261\n",
      "Updating learning rate to 1.9999999999999998e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 cost time: 2.466226816177368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.18it/s]\n",
      "7it [00:00, 17.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Train Loss: 0.4230974 Vali Loss: 0.2789089 Test Loss: 0.3581070 MAE Loss: 0.4573104\n",
      "Updating learning rate to 9.999999999999999e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 cost time: 2.4679837226867676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.59it/s]\n",
      "7it [00:00, 17.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Train Loss: 0.3775924 Vali Loss: 0.2501029 Test Loss: 0.3280290 MAE Loss: 0.4368432\n",
      "Updating learning rate to 4.9999999999999996e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 cost time: 2.4529926776885986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.35it/s]\n",
      "7it [00:00, 17.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Train Loss: 0.3563495 Vali Loss: 0.2400098 Test Loss: 0.3214180 MAE Loss: 0.4294241\n",
      "Updating learning rate to 2.4999999999999998e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 cost time: 2.454951286315918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.31it/s]\n",
      "7it [00:00, 17.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Train Loss: 0.3533191 Vali Loss: 0.2358763 Test Loss: 0.3191664 MAE Loss: 0.4262560\n",
      "Updating learning rate to 1.2499999999999999e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 cost time: 2.4345879554748535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.29it/s]\n",
      "7it [00:00, 17.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Train Loss: 0.3532724 Vali Loss: 0.2339333 Test Loss: 0.3180220 MAE Loss: 0.4246405\n",
      "Updating learning rate to 6.249999999999999e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 cost time: 2.461543083190918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.31it/s]\n",
      "7it [00:00, 17.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Train Loss: 0.3394702 Vali Loss: 0.2328657 Test Loss: 0.3171893 MAE Loss: 0.4237422\n",
      "Updating learning rate to 3.1249999999999997e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 cost time: 2.466899871826172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.32it/s]\n",
      "7it [00:00, 17.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Train Loss: 0.3491256 Vali Loss: 0.2323055 Test Loss: 0.3167143 MAE Loss: 0.4232861\n",
      "Updating learning rate to 1.5624999999999999e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 cost time: 2.4623541831970215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.50it/s]\n",
      "7it [00:00, 17.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Train Loss: 0.3464998 Vali Loss: 0.2320138 Test Loss: 0.3164621 MAE Loss: 0.4230461\n",
      "Updating learning rate to 7.812499999999999e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 cost time: 2.453521966934204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.30it/s]\n",
      "7it [00:00, 17.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Train Loss: 0.3472790 Vali Loss: 0.2318580 Test Loss: 0.3163286 MAE Loss: 0.4229203\n",
      "Updating learning rate to 3.9062499999999997e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 cost time: 2.4639735221862793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.33it/s]\n",
      "7it [00:00, 17.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Train Loss: 0.3411651 Vali Loss: 0.2317827 Test Loss: 0.3162585 MAE Loss: 0.4228554\n",
      "Updating learning rate to 1.9531249999999998e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 cost time: 2.453308582305908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.35it/s]\n",
      "7it [00:00, 17.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Train Loss: 0.3461599 Vali Loss: 0.2317455 Test Loss: 0.3162282 MAE Loss: 0.4228245\n",
      "Updating learning rate to 9.765624999999999e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 cost time: 2.4578335285186768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.31it/s]\n",
      "7it [00:00, 17.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Train Loss: 0.3422392 Vali Loss: 0.2317275 Test Loss: 0.3162167 MAE Loss: 0.4228105\n",
      "Updating learning rate to 4.8828124999999996e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 cost time: 2.456341505050659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.28it/s]\n",
      "7it [00:00, 17.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Train Loss: 0.3413221 Vali Loss: 0.2317184 Test Loss: 0.3162113 MAE Loss: 0.4228041\n",
      "Updating learning rate to 2.4414062499999998e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 cost time: 2.464506149291992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.26it/s]\n",
      "7it [00:00, 17.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Train Loss: 0.3468515 Vali Loss: 0.2317149 Test Loss: 0.3162086 MAE Loss: 0.4228014\n",
      "Updating learning rate to 1.2207031249999999e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 cost time: 2.4524543285369873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.25it/s]\n",
      "7it [00:00, 17.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Train Loss: 0.3467835 Vali Loss: 0.2317138 Test Loss: 0.3162077 MAE Loss: 0.4228004\n",
      "Updating learning rate to 6.103515624999999e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 cost time: 2.459472894668579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.24it/s]\n",
      "7it [00:00, 17.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Train Loss: 0.3467958 Vali Loss: 0.2317134 Test Loss: 0.3162073 MAE Loss: 0.4228001\n",
      "Updating learning rate to 3.0517578124999997e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 cost time: 2.4754796028137207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.25it/s]\n",
      "7it [00:00, 17.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Train Loss: 0.3446213 Vali Loss: 0.2317133 Test Loss: 0.3162072 MAE Loss: 0.4228000\n",
      "Updating learning rate to 1.5258789062499999e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:02, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 cost time: 2.4510536193847656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 14.26it/s]\n",
      "7it [00:00, 17.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Train Loss: 0.3485186 Vali Loss: 0.2317133 Test Loss: 0.3162072 MAE Loss: 0.4228000\n",
      "Updating learning rate to 7.629394531249999e-11\n",
      "success delete checkpoints\n"
     ]
    }
   ],
   "source": [
    "default_args = {\n",
    "    'task_name': 'short_term_forecast',\n",
    "    'is_training': 1,\n",
    "    'model_id': 'test',\n",
    "    'model_comment': 'none',\n",
    "    'seed': 2021,\n",
    "    'data': 'agri',\n",
    "    'root_path': './',\n",
    "    'data_path': 'raw_dat_v2.csv',\n",
    "    'features': 'S',\n",
    "    'target': 'MinT(°C)',\n",
    "    'loader': 'modal',\n",
    "    'checkpoints': './checkpoints/',\n",
    "    'seq_len': 48,\n",
    "    'label_len': 24,\n",
    "    'pred_len': 10,\n",
    "    'enc_in': 7,\n",
    "    'dec_in': 7,\n",
    "    'c_out': 7,\n",
    "    'd_model': 16,\n",
    "    'n_heads': 8,\n",
    "    'e_layers': 2,\n",
    "    'd_layers': 1,\n",
    "    'd_ff': 32,\n",
    "    'moving_avg': 25,\n",
    "    'factor': 1,\n",
    "    'dropout': 0.1,\n",
    "    'activation': 'gelu',\n",
    "    'output_attention': False,\n",
    "    'patch_len': 16,\n",
    "    'stride': 8,\n",
    "    'prompt_domain': 0,\n",
    "    'llm_model': 'BERT',\n",
    "    'llm_dim': 768, #4096 for Llama\n",
    "    'num_workers': 10,\n",
    "    'itr': 1,\n",
    "    'train_epochs': 20,\n",
    "    'align_epochs': 1,\n",
    "    'batch_size': 32,\n",
    "    'eval_batch_size': 1,\n",
    "    'patience': 10,\n",
    "    'learning_rate': 0.001,\n",
    "    'des': 'test',\n",
    "    'loss': 'MSE',\n",
    "    'lradj': 'type1',\n",
    "    'pct_start': 0.2,\n",
    "    'use_amp': False,\n",
    "    'llm_layers': 1,\n",
    "    'percent': 100\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#args = parser.parse_args()\n",
    "args = argparse.Namespace(**default_args)\n",
    "\n",
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_{}_{}'.format(\n",
    "        args.task_name,\n",
    "        args.model_id,\n",
    "        args.data,\n",
    "        args.features,\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.factor,\n",
    "        #args.embed,\n",
    "        args.des, ii)\n",
    "\n",
    "    train_data, train_loader = data_provider(args, 'train')\n",
    "    vali_data, vali_loader = data_provider(args, 'val')\n",
    "    test_data, test_loader = data_provider(args, 'test')\n",
    "\n",
    "    '''if args.model == 'Autoformer':\n",
    "        model = Autoformer.Model(args).float()\n",
    "    elif args.model == 'DLinear':\n",
    "        model = DLinear.Model(args).float()\n",
    "    else:'''\n",
    "\n",
    "\n",
    "    model = Model(args).float().to(device)\n",
    "\n",
    "    train_loader = [(data.to(device), target.to(device)) for data, target in train_loader]\n",
    "    test_loader = [(data.to(device), target.to(device)) for data, target in test_loader]\n",
    "    vali_loader = [(data.to(device), target.to(device)) for data, target in vali_loader]\n",
    "\n",
    "    path = os.path.join(args.checkpoints,\n",
    "                        setting + '-' + args.model_comment)  # unique checkpoint saving path\n",
    "    #args.content = load_content(args)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    time_now = time.time()\n",
    "\n",
    "    train_steps = len(train_loader)\n",
    "    early_stopping = EarlyStopping(patience=args.patience)\n",
    "\n",
    "    trained_parameters = []\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad is True:\n",
    "            trained_parameters.append(p)\n",
    "\n",
    "    model_optim = optim.Adam(trained_parameters, lr=args.learning_rate)\n",
    "\n",
    "    if args.lradj == 'COS':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(model_optim, T_max=20, eta_min=1e-8)\n",
    "    else:\n",
    "        scheduler = lr_scheduler.OneCycleLR(optimizer=model_optim,\n",
    "                                            steps_per_epoch=train_steps,\n",
    "                                            pct_start=args.pct_start,\n",
    "                                            epochs=args.train_epochs,\n",
    "                                            max_lr=args.learning_rate)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    mae_metric = nn.L1Loss()\n",
    "    ops_mint = []\n",
    "    prs_mint = []\n",
    "    t_loss_mint = []\n",
    "    v_loss_mint = []\n",
    "    for epoch in range(args.train_epochs):\n",
    "        iter_count = 0\n",
    "        train_loss = []\n",
    "\n",
    "        model.train()\n",
    "        epoch_time = time.time()\n",
    "        for i, (batch_x, batch_y) in tqdm(enumerate(train_loader)):\n",
    "            iter_count += 1\n",
    "            model_optim.zero_grad()\n",
    "\n",
    "            batch_x = batch_x.float()\n",
    "            batch_y = batch_y.float()\n",
    "            #batch_x_mark = batch_x_mark.float()\n",
    "            #batch_y_mark = batch_y_mark.float()\n",
    "\n",
    "            dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float()\n",
    "            dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float()\n",
    "\n",
    "            if args.output_attention:\n",
    "                outputs = model(batch_x, dec_inp)[0]\n",
    "            else:\n",
    "                outputs = model(batch_x, dec_inp)\n",
    "\n",
    "\n",
    "            f_dim = -1 if args.features == 'MS' else 0\n",
    "            outputs = outputs[:, -args.pred_len:, f_dim:]\n",
    "            batch_y = batch_y[:, -args.pred_len:, f_dim:]\n",
    "\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            model_optim.step()\n",
    "\n",
    "            if args.lradj == 'TST':\n",
    "                adjust_learning_rate(model_optim, scheduler, epoch + 1, args)\n",
    "\n",
    "        print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "        train_loss = np.average(train_loss)\n",
    "        t_loss_mint.append(train_loss)\n",
    "\n",
    "        vali_loss, vali_mae_loss,_,_ = vali(args, model, vali_data, vali_loader, criterion, mae_metric, vis_flag=False)\n",
    "        v_loss_mint.append(vali_loss)\n",
    "\n",
    "        if epoch == (args.train_epochs-1):\n",
    "            vis_flag = True\n",
    "            test_loss, test_mae_loss, ops_mint, prs_mint = vali(args, model, test_data, test_loader, criterion, mae_metric, vis_flag)\n",
    "            f_loss_mint = (test_loss, test_mae_loss)\n",
    "        else:\n",
    "            vis_flag = False\n",
    "            test_loss, test_mae_loss, _, _ = vali(args, model, test_data, test_loader, criterion, mae_metric, vis_flag)\n",
    "        \n",
    "\n",
    "        \n",
    "        print(\n",
    "            \"Epoch: {0} | Train Loss: {1:.7f} Vali Loss: {2:.7f} Test Loss: {3:.7f} MAE Loss: {4:.7f}\".format(\n",
    "                epoch + 1, train_loss, vali_loss, test_loss, test_mae_loss))\n",
    "\n",
    "        early_stopping(vali_loss, model, path)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        if args.lradj != 'TST':\n",
    "            if args.lradj == 'COS':\n",
    "                scheduler.step()\n",
    "                print(\"lr = {:.10f}\".format(model_optim.param_groups[0]['lr']))\n",
    "            else:\n",
    "                if epoch == 0:\n",
    "                    args.learning_rate = model_optim.param_groups[0]['lr']\n",
    "                    print(\"lr = {:.10f}\".format(model_optim.param_groups[0]['lr']))\n",
    "                adjust_learning_rate(model_optim, scheduler, epoch + 1, args, printout=True)\n",
    "\n",
    "        else:\n",
    "            print('Updating learning rate to {}'.format(scheduler.get_last_lr()[0]))\n",
    "\n",
    "if not os.path.exists('./checkpoints'):\n",
    "    os.makedirs('./checkpoints')  # unique checkpoint saving path\n",
    "del_files('./checkpoints')  # delete checkpoint files\n",
    "print('success delete checkpoints')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "o1_mint = ops_mint[0][0]\n",
    "p1_mint = prs_mint[0][0]\n",
    "for i in range(1,len(ops_mint)):\n",
    "    o1_mint = np.concatenate((o1_mint,ops_mint[1][0]))\n",
    "    p1_mint = np.concatenate((p1_mint,prs_mint[1][0]))\n",
    "\n",
    "o2_mint = test_data.scaler.inverse_transform(o1_mint.reshape(-1,1))\n",
    "p2_mint = test_data.scaler.inverse_transform(p1_mint.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig= plt.figure()\n",
    "\n",
    "# Plot o1\n",
    "plt.plot(o2_mint, label='Truth')\n",
    "\n",
    "# Plot p1\n",
    "plt.plot(p2_mint, label='Pred')\n",
    "\n",
    "# Labeling and title\n",
    "plt.xlabel('Week Index')\n",
    "plt.ylabel('MinT(°C)')\n",
    "plt.title('MinT(°C)Truth vs Pred')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot as an image (e.g., PNG)\n",
    "plt.savefig('plot_image_minT_20_epoch.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2)\n",
    "\n",
    "axs[0,0].plot(t_loss_rh, label='train')\n",
    "axs[0,0].plot(v_loss_rh, label='val')\n",
    "axs[0,0].set_title('RH(%)')\n",
    "axs[0,0].set_xlabel('Epoch')\n",
    "axs[0,0].set_ylabel('MSE Loss')\n",
    "\n",
    "axs[0,1].plot(t_loss_maxt, label='train')\n",
    "axs[0,1].plot(v_loss_maxt, label='val')\n",
    "axs[0,1].set_title('MaxT(°C)')\n",
    "axs[0,1].set_xlabel('Epoch')\n",
    "axs[0,1].set_ylabel('MSE Loss')\n",
    "\n",
    "axs[1,0].plot(t_loss_mint, label='train')\n",
    "axs[1,0].plot(v_loss_mint, label='val')\n",
    "axs[1,0].set_title('MinT(°C)')\n",
    "axs[1,0].set_xlabel('Epoch')\n",
    "axs[1,0].set_ylabel('MSE Loss')\n",
    "\n",
    "\n",
    "axs[1,1].plot(t_loss_rf, label='train')\n",
    "axs[1,1].plot(v_loss_rf, label='val')\n",
    "axs[1,1].set_title('RF(mm)')\n",
    "axs[1,1].set_xlabel('Epoch')\n",
    "axs[1,1].set_ylabel('MSE Loss')\n",
    "\n",
    "fig.suptitle('Train, Val Loss vs Epoch', fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('epoch_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig= plt.figure()\n",
    "\n",
    "f_loss = [f_loss_rh, f_loss_maxt, f_loss_mint, f_loss_rf]\n",
    "\n",
    "mse_loss = [i[0] for i in f_loss]\n",
    "mae_loss = [i[1] for i in f_loss]\n",
    "features = ['RH(%)', 'MaxT(°C)', 'MinT(°C)', 'RF(mm)']\n",
    "\n",
    "# Set the width of the bars\n",
    "bar_width = 0.35\n",
    "\n",
    "# Set the position of the bars on the x-axis\n",
    "r1 = np.arange(len(mse_loss))\n",
    "r2 = [x + bar_width for x in r1]\n",
    "\n",
    "# Create the bar plot\n",
    "plt.bar(r1, mse_loss, color='b', width=bar_width, edgecolor='grey', label='MSE Loss')\n",
    "plt.bar(r2, mae_loss, color='r', width=bar_width, edgecolor='grey', label='MAE Loss')\n",
    "\n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xlabel('Features', fontweight='bold')\n",
    "plt.xticks([r + bar_width/2 for r in range(len(mse_loss))], features)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Comparison of MSE Loss and MAE Loss for Different Parameters')\n",
    "plt.savefig('test_loss.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
